{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notebook, we give the example of training the cluster-level predictor that is introduced in the paper. For other baseline models, you can change the import (for )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from model import CrowdPredictor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "import os\n",
    "from multiprocessing import pool\n",
    "\n",
    "from config import didi_hidx_traj_path, num_clusters, num_timeslots, time_embedding, cluster_embedding, hidden_dim, context_dim, dT, context_max_predictor_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowd_predictor = CrowdPredictor(num_timeslots, time_embedding, num_clusters, cluster_embedding, hidden_dim, context_dim, n_layers=2, pooling_func='max').cuda(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121001.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121002.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121003.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121004.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121005.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121006.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121007.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121008.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121009.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121010.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121011.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121012.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121013.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121014.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121015.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121016.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121017.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121018.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121019.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121020.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121021.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121022.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121023.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121024.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121025.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121026.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121027.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121028.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121029.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121030.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121031.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121101.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121102.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121103.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121104.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121105.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121106.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121107.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121108.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121109.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121110.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121111.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121112.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121113.pk\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121114.pk\n"
     ]
    }
   ],
   "source": [
    "def load_cluster_data(filepath):\n",
    "    print(filepath)\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return torch.stack(list(pk.load(f).values()))\n",
    "\n",
    "path_list = []\n",
    "\n",
    "for m in range(10, 12):\n",
    "    for d in range(1, 32):\n",
    "        filename = os.path.join(didi_hidx_traj_path, f'hidx_traj_2012{m:02d}{d:02d}.pk')\n",
    "        if os.path.isfile(filename):\n",
    "            path_list.append(filename)\n",
    "\n",
    "path_list = path_list[:45]\n",
    "\n",
    "data = [load_cluster_data(filepath) for filepath in path_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(crowd_predictor.parameters(), lr=1e-3)\n",
    "optimizer_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001, 100.0%, avg_loss = 0.6110\n",
      "Epoch 002, 100.0%, avg_loss = 0.4951\n",
      "Epoch 003, 100.0%, avg_loss = 0.4883\n",
      "Epoch 004, 100.0%, avg_loss = 0.4850\n",
      "Epoch 005, 100.0%, avg_loss = 0.4836\n",
      "Epoch 006, 100.0%, avg_loss = 0.4826\n",
      "Epoch 007, 100.0%, avg_loss = 0.4824\n",
      "Epoch 008, 100.0%, avg_loss = 0.4820\n",
      "Epoch 009, 100.0%, avg_loss = 0.4818\n",
      "Epoch 010, 100.0%, avg_loss = 0.4821\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8192\n",
    "max_iter = 100000\n",
    "\n",
    "day_time_list = list(product(range(len(data)), range(num_timeslots - 2 * dT + 1)))\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    avg_loss = 0.0\n",
    "    random.shuffle(day_time_list)\n",
    "\n",
    "    i = 0\n",
    "    for d, t in day_time_list:\n",
    "\n",
    "        i += 1\n",
    "        batch_indices = torch.LongTensor(np.random.choice(data[d].shape[0], batch_size, replace=False))        \n",
    "        xc = data[d][batch_indices][:, t: t + dT].cuda(1)\n",
    "        xt = torch.zeros_like(xc) + t\n",
    "        yc = data[d][batch_indices, t + 2 * dT - 1].cuda(1)\n",
    "\n",
    "        crowd_pred = crowd_predictor(xc, xt)\n",
    "        loss = nn.functional.cross_entropy(crowd_pred, yc)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        print('Epoch {:03d}, {:.1f}%, avg_loss = {:.4f}'.format(epoch, i / len(day_time_list) * 100, avg_loss / i), end='\\r')\n",
    "        \n",
    "    print()\n",
    "    optimizer_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fan/anaconda3/envs/py3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type CrowdPredictor. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/fan/anaconda3/envs/py3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/fan/anaconda3/envs/py3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/fan/anaconda3/envs/py3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/fan/anaconda3/envs/py3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/fan/anaconda3/envs/py3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(crowd_predictor.cpu(), context_max_predictor_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 (Conda)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
