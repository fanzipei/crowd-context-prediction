{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "\n",
    "from model import GRUPredictor\n",
    "\n",
    "from multiprocessing import pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ensemble_components_folder_path\n",
    "from config import didi_hidx_traj_path\n",
    "from config import num_clusters, num_timeslots, time_embedding, cluster_embedding, hidden_dim, context_dim, dT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import gru_predictor_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_cluster_data(filepath):\n",
    "    print(filepath)\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return torch.stack(list(pk.load(f).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cluster_data(filepath):\n",
    "    print(filepath)\n",
    "    with open(filepath, 'rb') as f:\n",
    "        return torch.stack(list(pk.load(f).values()))\n",
    "\n",
    "path_list = []\n",
    "\n",
    "for m in range(10, 12):\n",
    "    for d in range(1, 32):\n",
    "        filename = os.path.join(didi_hidx_traj_path, f'hidx_traj_2012{m:02d}{d:02d}.pk')\n",
    "        if os.path.isfile(filename):\n",
    "            path_list.append((m, d, filename))\n",
    "\n",
    "path_list = path_list[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(ensemble_components_folder_path):\n",
    "    os.mkdir(ensemble_components_folder_path)\n",
    "    print('Create folder', ensemble_components_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121001.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4566\n",
      "Epoch 002, 100.0%, avg_loss = 0.4503\n",
      "Epoch 003, 100.0%, avg_loss = 0.4502\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121002.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4482\n",
      "Epoch 002, 100.0%, avg_loss = 0.4438\n",
      "Epoch 003, 100.0%, avg_loss = 0.4414\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121003.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4492\n",
      "Epoch 002, 100.0%, avg_loss = 0.4413\n",
      "Epoch 003, 100.0%, avg_loss = 0.4416\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121004.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4449\n",
      "Epoch 002, 100.0%, avg_loss = 0.4405\n",
      "Epoch 003, 100.0%, avg_loss = 0.4384\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121005.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4420\n",
      "Epoch 002, 100.0%, avg_loss = 0.4392\n",
      "Epoch 003, 100.0%, avg_loss = 0.4338\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121006.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4488\n",
      "Epoch 002, 100.0%, avg_loss = 0.4416\n",
      "Epoch 003, 100.0%, avg_loss = 0.4405\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121007.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4501\n",
      "Epoch 002, 100.0%, avg_loss = 0.4450\n",
      "Epoch 003, 100.0%, avg_loss = 0.4433\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121008.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4680\n",
      "Epoch 002, 100.0%, avg_loss = 0.4637\n",
      "Epoch 003, 100.0%, avg_loss = 0.4608\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121009.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4648\n",
      "Epoch 002, 100.0%, avg_loss = 0.4598\n",
      "Epoch 003, 100.0%, avg_loss = 0.4599\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121010.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4580\n",
      "Epoch 002, 100.0%, avg_loss = 0.4528\n",
      "Epoch 003, 100.0%, avg_loss = 0.4510\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121011.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4554\n",
      "Epoch 002, 100.0%, avg_loss = 0.4533\n",
      "Epoch 003, 100.0%, avg_loss = 0.4520\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121012.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4579\n",
      "Epoch 002, 100.0%, avg_loss = 0.4569\n",
      "Epoch 003, 100.0%, avg_loss = 0.4548\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121013.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4519\n",
      "Epoch 002, 100.0%, avg_loss = 0.4475\n",
      "Epoch 003, 100.0%, avg_loss = 0.4465\n",
      "/data/fan/didi/processed/hidx_traj/hidx_traj_20121014.pk\n",
      "Epoch 001, 100.0%, avg_loss = 0.4541\n",
      "Epoch 002, 100.0%, avg_loss = 0.4492\n",
      "Epoch 003, 100.0%, avg_loss = 0.4479\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "max_iter = 10000\n",
    "\n",
    "for m, d, filename in path_list:\n",
    "    data = load_cluster_data(filename)\n",
    "    \n",
    "    predictor = torch.load(gru_predictor_model_path).cuda(1)\n",
    "    optimizer = torch.optim.RMSprop(predictor.parameters(), lr=1e-5)\n",
    "    optimizer_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "\n",
    "    for epoch in range(1, 4):\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for i in range(1, 1 + max_iter):\n",
    "\n",
    "            i += 1\n",
    "            t = np.random.randint(num_timeslots - 2 * dT + 1)\n",
    "            \n",
    "            batch_indices = torch.LongTensor(np.random.choice(data.shape[0], batch_size, replace=False))        \n",
    "            xc = data[batch_indices][:, t: t + dT].cuda(1)\n",
    "            xt = torch.zeros_like(xc) + t\n",
    "            yc = data[batch_indices, t + 2 * dT - 1].cuda(1)\n",
    "\n",
    "            pred = predictor(xc, xt)\n",
    "            loss = nn.functional.cross_entropy(pred, yc)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            print('Epoch {:03d}, {:.1f}%, avg_loss = {:.4f}'.format(epoch, i / max_iter * 100, avg_loss / i), end='\\r')\n",
    "\n",
    "        print()\n",
    "        optimizer_scheduler.step()\n",
    "    torch.save(predictor.cpu(), f'{ensemble_components_folder_path}ensemble_gru_day_{d:02d}.pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 (Conda)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
